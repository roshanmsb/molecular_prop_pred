{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.utils.util_funcs import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (2335,), y.shape: (2335, 1), w.shape: (2335, 1), task_names: ['ecoli']>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset('ecoli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "\n",
    "class MultilabelBalancedRandomSampler(Sampler):\n",
    "    \"\"\"\n",
    "    MultilabelBalancedRandomSampler: Given a multilabel dataset of length n_samples and\n",
    "    number of classes n_classes, samples from the data with equal probability per class\n",
    "    effectively oversampling minority classes and undersampling majority classes at the\n",
    "    same time. Note that using this sampler does not guarantee that the distribution of\n",
    "    classes in the output samples will be uniform, since the dataset is multilabel and\n",
    "    sampling is based on a single class. This does however guarantee that all classes\n",
    "    will have at least batch_size / n_classes samples as batch_size approaches infinity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, indices=None, class_choice=\"least_sampled\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            labels: a multi-hot encoding numpy array of shape (n_samples, n_classes)\n",
    "            indices: an arbitrary-length 1-dimensional numpy array representing a list\n",
    "            of indices to sample only from\n",
    "            class_choice: a string indicating how class will be selected for every\n",
    "            sample:\n",
    "                \"least_sampled\": class with the least number of sampled labels so far\n",
    "                \"random\": class is chosen uniformly at random\n",
    "                \"cycle\": the sampler cycles through the classes sequentially\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        if self.indices is None:\n",
    "            self.indices = range(len(labels))\n",
    "\n",
    "        self.num_classes = self.labels.shape[1]\n",
    "\n",
    "        # List of lists of example indices per class\n",
    "        self.class_indices = []\n",
    "        for class_ in range(self.num_classes):\n",
    "            lst = np.where(self.labels[:, class_] == 1)[0]\n",
    "            lst = lst[np.isin(lst, self.indices)]\n",
    "            self.class_indices.append(lst)\n",
    "\n",
    "        self.counts = [0] * self.num_classes\n",
    "\n",
    "        assert class_choice in [\"least_sampled\", \"random\", \"cycle\"]\n",
    "        self.class_choice = class_choice\n",
    "        self.current_class = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count >= len(self.indices):\n",
    "            raise StopIteration\n",
    "        self.count += 1\n",
    "        return self.sample()\n",
    "\n",
    "    def sample(self):\n",
    "        class_ = self.get_class()\n",
    "        class_indices = self.class_indices[class_]\n",
    "        chosen_index = np.random.choice(class_indices)\n",
    "        if self.class_choice == \"least_sampled\":\n",
    "            for class_, indicator in enumerate(self.labels[chosen_index]):\n",
    "                if indicator == 1:\n",
    "                    self.counts[class_] += 1\n",
    "        return chosen_index\n",
    "\n",
    "    def get_class(self):\n",
    "        if self.class_choice == \"random\":\n",
    "            class_ = random.randint(0, self.labels.shape[1] - 1)\n",
    "        elif self.class_choice == \"cycle\":\n",
    "            class_ = self.current_class\n",
    "            self.current_class = (self.current_class + 1) % self.labels.shape[1]\n",
    "        elif self.class_choice == \"least_sampled\":\n",
    "            min_count = self.counts[0]\n",
    "            min_classes = [0]\n",
    "            for class_ in range(1, self.num_classes):\n",
    "                if self.counts[class_] < min_count:\n",
    "                    min_count = self.counts[class_]\n",
    "                    min_classes = [class_]\n",
    "                if self.counts[class_] == min_count:\n",
    "                    min_classes.append(class_)\n",
    "            class_ = np.random.choice(min_classes)\n",
    "        return class_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem.molnet as dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dcm.load_bbbp(featurizer='raw',splitter=None)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = MultilabelBalancedRandomSampler(data.y,class_choice=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolfiles import MolFromSmiles\n",
    "import pandas as pd\n",
    "from deepchem.data import DiskDataset\n",
    "from rdkit import RDLogger\n",
    "import tokenizers\n",
    "\n",
    "from typing import Callable, Dict, List\n",
    "from rdkit import RDLogger\n",
    "import numpy as np\n",
    "from deepchem.data.datasets import DiskDataset\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "def smiles2index(s_1: str, tokenizer: tokenizers.Tokenizer) -> List[int]:\n",
    "    \"\"\"Tokenize a SMILES string\n",
    "\n",
    "    Args:\n",
    "        s_1 (str): SMILES string\n",
    "        tokenizer (tokenizers.Tokenizer): Pretrained tokenizer\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List of tokens\n",
    "    \"\"\"\n",
    "    return tokenizer.encode(s_1).ids\n",
    "\n",
    "\n",
    "def index2multi_hot_fg(molecule: Chem.rdchem.Mol, fgroups_list: List[str]) -> np.ndarray:\n",
    "    \"\"\"Generate functional group representation\n",
    "\n",
    "    Args:\n",
    "        molecule (Chem.rdchem.Mol): Rdkit molecule from SMILES string\n",
    "        fgroups_list (List[str]): List of SMARTS strings for functional groups\n",
    "\n",
    "    Returns:\n",
    "        List[int]: One hot encoding of functional groups\n",
    "    \"\"\"\n",
    "    v_1 = np.zeros(len(fgroups_list))\n",
    "    for idx, f_g in enumerate(fgroups_list):\n",
    "        if molecule.HasSubstructMatch(f_g):\n",
    "            v_1[idx] = 1\n",
    "    return v_1\n",
    "\n",
    "\n",
    "def smiles2vector_fgr(\n",
    "    s_1: str, tokenizer: tokenizers.Tokenizer, fgroups_list: List[str]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate Functional Groups (FG) and Mined Functional Groups (MFG)\n",
    "\n",
    "    Args:\n",
    "        s_1 (str): SMILES string\n",
    "        tokenizer (tokenizers.Tokenizer): Pretrained tokenizer\n",
    "        fgroups_list (List[str]): List of SMARTS strings for functional groups\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int],List[int]]: FG and MFG\n",
    "    \"\"\"\n",
    "    i_1 = smiles2index(s_1, tokenizer)\n",
    "    mfg = np.zeros(tokenizer.get_vocab_size())\n",
    "    mfg[i_1] = 1\n",
    "    molecule = MolFromSmiles(s_1)\n",
    "    f_g = index2multi_hot_fg(molecule, fgroups_list)\n",
    "    return f_g, mfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGRDataset(Dataset):\n",
    "    \"\"\"Pytorch dataset for training and testing different models\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: DiskDataset,\n",
    "        fgroups_list: List[str],\n",
    "        tokenizer: Tokenizer,\n",
    "        descriptor_funcs: Dict[str, Callable],\n",
    "    ) -> None:\n",
    "        \"\"\"Initiliaze dataset with arguments\n",
    "\n",
    "        Args:\n",
    "            data (DiskDataset): Deepchem dataset containing SMILES and labels\n",
    "            fgroups_list (List[str]): List of functional groups\n",
    "            tokenizer (Tokenizer): Pretrained tokenizer\n",
    "            descriptor_funcs (Dict[str, Callable]): RDKit descriptor dictionary\n",
    "        \"\"\"\n",
    "        self.mols = data.X\n",
    "        self.labels = data.y\n",
    "        self.smiles = data.ids\n",
    "        self.fgroups_list = fgroups_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.descriptor_funcs = descriptor_funcs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mol = self.mols[idx]\n",
    "        smile = self.smiles[idx]\n",
    "        target = self.labels[idx]\n",
    "        f_g, mfg = smiles2vector_fgr(smile, self.tokenizer, self.fgroups_list)\n",
    "        num_features = np.asarray(\n",
    "            [self.descriptor_funcs[key](mol) for key in self.descriptor_funcs.keys()]\n",
    "        )\n",
    "        return f_g, mfg, num_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List\n",
    "from rdkit.Chem.rdmolfiles import MolFromSmarts\n",
    "from rdkit.Chem import Descriptors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deepchem.splits.splitters import RandomStratifiedSplitter, ScaffoldSplitter\n",
    "import deepchem.molnet as dcm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "\n",
    "descriptor_funcs = {name: func for name, func in Descriptors.descList}\n",
    "fgroups = pd.read_csv(\"../datasets/processed/fg.csv\")[\"SMARTS\"].tolist()\n",
    "fgroups_list = [MolFromSmarts(x) for x in fgroups]\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\")).from_file(\"tokenizer_bpe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FGRDataset(data, fgroups_list,tokenizer, descriptor_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([ 6, 58]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(next(iter(lo))[-1],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([ 479, 1560]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data.y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(data.w, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading SMILES\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\n",
       "1                             CC(=O)OC(CC(=O)O)C[N+](C)(C)C\n",
       "2                                  C1=CC(C(C(=C1)C(=O)O)O)O\n",
       "3                                                   CC(CN)O\n",
       "4                                      C(C(=O)COP(=O)(O)O)N\n",
       "                                ...                        \n",
       "441734    C[C@@H]1[C@H]([C@@]2([C@@H](C2(C)C)[C@H]3[C@]1...\n",
       "441735    C[C@@H]1[C@H]([C@@]2([C@@H](C2(C)C)[C@H]3[C@]1...\n",
       "441736    CC[C@H](C)C(=O)O[C@@]12[C@@H](C1(C)C)[C@@H]3C=...\n",
       "441737    CC[C@H](C)C(=O)O[C@@]12[C@@H](C1(C)C)[C@@H]3C=...\n",
       "441738    CC[C@H](C)C(=O)O[C@@]12[C@@H](C1(C)C)[C@@H]3C=...\n",
       "Name: SMILES, Length: 100903545, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "361a8d57a0e2a78723716f42fcaf9253c19957bfb2980caf149704a44c60f9de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
